### 问题:
我现在有一堆网站的url，我想下载这些网站的内容


### 解决办法:

> 异步方式:
```
import asyncio
import aiohttp

urls = ['http://baidu.com'] * 100
print(urls)

async def get_url_content(url):
    async with aiohttp.request('GET', url) as resp:
            data = await resp.text()
            return data

loop = asyncio.get_event_loop()
tasks = [asyncio.ensure_future(get_url_content(urls[i])) for i in range(len(urls))]
loop.run_until_complete(asyncio.wait(tasks))
loop.close()

print('task result', '#'*100)
for task in tasks:
    print(task.result())
```
